<!DOCTYPE HTML>
<!--
	DANH NGUYEN THANH Profile
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Nguyen Phan - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" type="image/ico" href="/images/logo/favicon.ico" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Nguyen's Portfolio</a></h1>
						<nav class="links">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="cv.html">Curriculum Vitae</a></li>
								<li><a href="research.html">Research</a></li>
								<li><a href="projects.html"><b>Projects</b></a></li>
								<!-- <li><a href="materials.html">Materials</a></li> -->
								<li><a href="contact.html">Contact</a></li>
								


							</ul>
						</nav>
						<nav class="main">
							<ul>
								<li class="menu">
									<a class="fa-bars" href="#menu">Menu</a>
								</li>
							</ul>
						</nav>
					</header>

				<!-- Menu -->
					<section id="menu">

						<!-- Links -->
							<section>
								<ul class="links">
									<li><a href="index.html">Home</a></li>
									<li><a href="cv.html">Curriculum Vitae</a></li>
									<li><a href="research.html">Research</a></li>
									<li><a href="projects.html"><b>Projects</b></a></li>
									<!-- <li><a href="materials.html">Materials</a></li> -->
									<li><a href="contact.html">Contact</a></li>
								</ul>
							</section>

					</section>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
						<article class="post">
							<!-- <header>
								<div class="title">
									<h2><a href="#">Project List</a></h2>
								</div>
							</header> -->
							<ol>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#CAMO-FS">LoGoViT</a></h2></li>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#InstSynth">APA</a></h2></li>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#LTSP">Label Transfer Scene Parser</a></h2></li>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#CE-OST">Missing Item</a></h2></li>
							</ol>
						</article>

						<!-- Post -->
						<article class="post" id="CAMO-FS">
							<header>
								<div class="title">
									<h2><a href="#">LoGoViT</a></h2>
									<p>Few-shot Learning for Animal Detection and Segmentation</p>
								</div>
								<div class="meta">
									<time class="published">Jul 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\camofs_project.jpg" alt=""  /></a>

								<p align="justify">Object reidentification (ReID) is prone to errors under variations in scale, illumination, complex background, and ob- ject occlusion scenarios. 
									To overcome these challenges, attention mechanisms are employed to focus on the object’s characteristics, thereby extracting better discriminative features. 
									This paper introduces a local-global vision transformer (LoGoViT) for object reidentification by learning a hierarchical-level representation from fine-grained (local) to general (global) context features. 
									It comprises two com- ponents: (i) shift and shuffle operations to generate robust local features and (ii) local-global module to aggregate the multi-level hierarchy features of an object. 
									Extensive experiments show that our method achieves state-of-the-art on the ReID benchmarks. 
									We further investigate effective augmentation operations and discuss how the patch modifications improve the proposed model’s generalization under occlusion scenarios.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/nguyenphan99/LoGoViT" class="button medium">Code</a>
								</p>
									
								<b>Reference</b>:
								<blockquote>
									<b>Nguyen Phan</b> ,Ta Duc Huy, Soan T. M. Duong, Nguyen Tran Hoang, Sam Tran, 
									Dao Huu Hung, Chanh D. Tr. Nguyen, Trung Bui, Steven Q. H. Truong, “LOGOVIT: LOCAL-GLOBAL VISION TRANSFORMER FOR OBJECT RE-IDENTIFICATION”,
                                        ICASSP, 2023. 
<!-- 										[<a style="color:rgb(0, 85, 255);" href="https://ieeexplore.ieee.org/document/10608133">DOI</a>,
										<a style="color:rgb(0, 85, 255);" href="https://arxiv.org/abs/2304.07444">ArXiv</a>] -->
								</blockquote>
						</article>

						<!-- Post -->
						

						<!-- Post -->
						<article class="post" id="LTSP">
							<header>
								<div class="title">
									<h2><a href="#">Label Transfer Scene Parser</a></h2>
									<p>Nighttime Scene Understanding</p>
								</div>
								<div class="meta">
									<time class="published">Sep 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\ltsp_project.jpg" alt=""  /></a>

								<p align="justify">Semantic segmentation plays a crucial role in traffic scene understanding, especially in nighttime condition. 
								This paper tackles the task of semantic segmentation on nighttime scenes. The largest challenge of this task is the lack of 
								annotated nighttime images to train a deep learning-based scene parser. The existing annotated datasets are abundant in daytime 
								condition but scarce in nighttime due to the high cost. Thus, we propose a novel Label Transfer Scene Parser (LTSP) framework 
								for nighttime scene semantic segmentation by leveraging daytime annotation transfer. Our framework performs segmentation in 
								the dark without training on real nighttime annotated data. In particular, we propose translating daytime images to 
								nighttime condition to obtain more data with annotation in an efficient way. In addition, we utilize the pseudo-labels 
								inferred from unlabeled nighttime scenes to further train the scene parser. The novelty of our work is the ability to 
								perform nighttime segmentation via daytime annotated label and nighttime synthetic versions of the same set of images. 
								The extensive experiments demonstrate the improvement and efficiency of our scene parser over the state-of-the-art methods 
								with the similar semi-supervised approach on the benchmark of Nighttime Driving Test dataset. Notably, our proposed method 
								utilizes only one tenth of the amount of labeled and unlabeled data in comparison with the previous methods.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/danhntd/Label_Transfer_Scene_Parser" class="button medium">Code</a>
								</p>

								<b>Reference</b>:
								<blockquote>
									<b>Thanh-Danh Nguyen</b>, Nguyen Phan, Tam V. Nguyen†, Vinh-Tiep Nguyen, and Minh-Triet Tran, 
										“Nighttime Scene Understanding with Label Transfer Scene Parser”,
                                        Image and Vision Computing, Sep 2024.
										[<a style="color:rgb(0, 85, 255);" href="https://doi.org/10.1016/j.imavis.2024.105257">DOI</a>]
								</blockquote>
						</article>

						<!-- Footer -->
						<section id="footer">
							<ul class="icons">

							</ul>
							<p class="copyright">&copy; T-Danh Nguyen</a>. Design: <a href="http://html5up.net">HTML5 UP</a>.</p>
						</section>
					</div>

				<!-- Sidebar -->
					<section id="sidebar">

						<!-- Intro -->
						<section id="intro">
							<a href="#" class="logo"><img src="images/logo.jpg" alt="" /></a>
							<header>
								<h2>Projects</h2>
							</header>
						</section>


						<section style="color:rgb(141, 141, 141);" class="blurb">
							<h2>News</h2>
						<li>A journal named Nighttime scene understanding with label transfer scene parse is accepted to IMAVIS Journal (SCIE-Q1), 2024.</li>
									<li>A paper named LoGoViT: Local-Global Vision Transformer for Object Re-identification is accepted to ICASSP, 2023. </li>
									<li>A paper about Abstraction-Perception Preserving Cartoon Face Synthesis is accepted MTA Journal (ISI-Q1), 2023. </li>
									<li>A paper named Improving Local Features with Relevant Spatial Information by Vision Transformer for Crowd Counting is accepted BMVC, 2022. </li>
									<li>A paper named Adaptive Proxy Anchor Loss for Deep Metric Learning is accepted to ICIP, 2022. </li>
									<li>A journal named Adaptive Multi-Vehicle Motion Counting is accepted to SIViP Journal (SCIE-Q2), 2022. </li>
									<li>A paper named ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue System Development is accepted to ICONIP, 2021. </li>


							<!-- <ul class="actions">
								<li><a href="#" class="button">Learn More</a></li>
							</ul> -->
						</section>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
