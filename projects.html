<!DOCTYPE HTML>
<!--
	DANH NGUYEN THANH Profile
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Nguyen Phan - Homepage</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="icon" type="image/ico" href="/images/logo/favicon.ico" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1><a href="index.html">Nguyen's Portfolio</a></h1>
						<nav class="links">
							<ul>
								<li><a href="index.html">Home</a></li>
								<li><a href="cv.html">Curriculum Vitae</a></li>
								<li><a href="research.html">Research</a></li>
								<li><a href="projects.html"><b>Projects</b></a></li>
								<!-- <li><a href="materials.html">Materials</a></li> -->
								<li><a href="contact.html">Contact</a></li>
								


							</ul>
						</nav>
						<nav class="main">
							<ul>
								<li class="menu">
									<a class="fa-bars" href="#menu">Menu</a>
								</li>
							</ul>
						</nav>
					</header>

				<!-- Menu -->
					<section id="menu">

						<!-- Links -->
							<section>
								<ul class="links">
									<li><a href="index.html">Home</a></li>
									<li><a href="cv.html">Curriculum Vitae</a></li>
									<li><a href="research.html">Research</a></li>
									<li><a href="projects.html"><b>Projects</b></a></li>
									<!-- <li><a href="materials.html">Materials</a></li> -->
									<li><a href="contact.html">Contact</a></li>
								</ul>
							</section>

					</section>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
						<article class="post">
							<!-- <header>
								<div class="title">
									<h2><a href="#">Project List</a></h2>
								</div>
							</header> -->
							<ol>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#CAMO-FS">LoGoViT</a></h2></li>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#InstSynth">Adaptive Proxy Anchor</a></h2></li>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#LTSP">Label Transfer Scene Parser</a></h2></li>
							<li><h2><a style="color:rgb(0, 85, 255);" href="#CE-OST">Missing Item</a></h2></li>
							</ol>
						</article>

						<!-- Post -->
						<article class="post" id="CAMO-FS">
							<header>
								<div class="title">
									<h2><a href="#">LoGoViT</a></h2>
									<p>LOCAL-GLOBAL VISION TRANSFORMER FOR OBJECT RE-IDENTIFICATION</p>
								</div>
								<div class="meta">
									<time class="published">Jul 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\camofs_project.jpg" alt=""  /></a>

								<p align="justify">Object reidentification (ReID) is prone to errors under variations in scale, illumination, complex background, and ob- ject occlusion scenarios. 
									To overcome these challenges, attention mechanisms are employed to focus on the object’s characteristics, thereby extracting better discriminative features. 
									This paper introduces a local-global vision transformer (LoGoViT) for object reidentification by learning a hierarchical-level representation from fine-grained (local) to general (global) context features. 
									It comprises two com- ponents: (i) shift and shuffle operations to generate robust local features and (ii) local-global module to aggregate the multi-level hierarchy features of an object. 
									Extensive experiments show that our method achieves state-of-the-art on the ReID benchmarks. 
									We further investigate effective augmentation operations and discuss how the patch modifications improve the proposed model’s generalization under occlusion scenarios.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/nguyenphan99/LoGoViT" class="button medium">Code</a>
								</p>
									
								<b>Reference</b>:
								<blockquote>
									<b>Nguyen Phan</b> ,Ta Duc Huy, Soan T. M. Duong, Nguyen Tran Hoang, Sam Tran, 
									Dao Huu Hung, Chanh D. Tr. Nguyen, Trung Bui, Steven Q. H. Truong, “LOGOVIT: LOCAL-GLOBAL VISION TRANSFORMER FOR OBJECT RE-IDENTIFICATION”,
                                        ICASSP, 2023. 
<!-- 										[<a style="color:rgb(0, 85, 255);" href="https://ieeexplore.ieee.org/document/10608133">DOI</a>,
										<a style="color:rgb(0, 85, 255);" href="https://arxiv.org/abs/2304.07444">ArXiv</a>] -->
								</blockquote>
						</article>

						<!-- Post -->
						

						<!-- Post -->
						<article class="post" id="LTSP">
							<header>
								<div class="title">
									<h2><a href="#">Label Transfer Scene Parser</a></h2>
									<p>Adaptive Proxy Anchor Loss for Deep Metric Learning</p>
								</div>
								<div class="meta">
									<time class="published">Sep 2024</time>
								</div>
							</header>
								<a href="#" class="image featured"><img src="images\ltsp_project.jpg" alt=""  /></a>

								<p align="justify">Deep metric learning (or simply called metric learning) uses the deep neural network to learn the representation of images, 
									leading to widely used in many applications, e.g. image retrieval and face recognition. 
									In the metric learning approaches, proxy anchor takes advantage of proxy-based and pair-based approaches to 
									enable fast convergence time and robustness to noisy labels. However, in training the proxy anchor, 
									selecting the hyperparameter margin is important to achieve a good performance. This selection requires expertise and is time-consuming. 
									This paper proposes a novel method to learn the margin while training the proxy anchor approach adaptively. 
									The proposed adaptive proxy anchor simplifies the hyperparameter tuning process while advancing the proxy anchor. 
									We achieve state of the art on three public datasets with a noticeably faster convergence time.</p>
								<p align="centering">
									<a style="color:rgb(0, 85, 255);" href="https://github.com/tks1998/Adaptive-Proxy-Anchor" class="button medium">Code</a>
								</p>

								<b>Reference</b>:
								<blockquote>
									<b>Nguyen Phan</b>*, Sen Tran, Ta Duc Huy, Soan T. M. Duong, and Chanh D. Tr. Nguyen, Trung Bui, Steven Q.H. Truong,
										“Adaptive Proxy Anchor Loss for Deep Metric Learning”,
                                        ICIP, 2022.
<!-- 										[<a style="color:rgb(0, 85, 255);" href="https://doi.org/10.1016/j.imavis.2024.105257">DOI</a>] -->
								</blockquote>
						</article>

						<!-- Footer -->
						<section id="footer">
							<ul class="icons">

							</ul>
							<p class="copyright">&copy; T-Danh Nguyen</a>. Design: <a href="http://html5up.net">HTML5 UP</a>.</p>
						</section>
					</div>

				<!-- Sidebar -->
					<section id="sidebar">

						<!-- Intro -->
						<section id="intro">
							<a href="#" class="logo"><img src="images/logo.jpg" alt="" /></a>
							<header>
								<h2>Projects</h2>
							</header>
						</section>


						<section style="color:rgb(141, 141, 141);" class="blurb">
							<h2>News</h2>
						<li>A journal named Nighttime scene understanding with label transfer scene parse is accepted to IMAVIS Journal (SCIE-Q1), 2024.</li>
									<li>A paper named LoGoViT: Local-Global Vision Transformer for Object Re-identification is accepted to ICASSP, 2023. </li>
									<li>A paper about Abstraction-Perception Preserving Cartoon Face Synthesis is accepted MTA Journal (ISI-Q1), 2023. </li>
									<li>A paper named Improving Local Features with Relevant Spatial Information by Vision Transformer for Crowd Counting is accepted BMVC, 2022. </li>
									<li>A paper named Adaptive Proxy Anchor Loss for Deep Metric Learning is accepted to ICIP, 2022. </li>
									<li>A journal named Adaptive Multi-Vehicle Motion Counting is accepted to SIViP Journal (SCIE-Q2), 2022. </li>
									<li>A paper named ViMQ: A Vietnamese Medical Question Dataset for Healthcare Dialogue System Development is accepted to ICONIP, 2021. </li>


							<!-- <ul class="actions">
								<li><a href="#" class="button">Learn More</a></li>
							</ul> -->
						</section>
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
